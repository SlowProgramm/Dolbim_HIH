import json
import re
import requests
import time
from bs4 import BeautifulSoup
from bs4.element import NavigableString, Tag
from tqdm import tqdm

BASE_CATALOG_URL = "https://www.rustore.ru/catalog/games/all"
MAX_PAGES = 4
TARGET_CATEGORIES = [
    "–®—É—Ç–µ—Ä—ã",
    "–ê—Ä–∫–∞–¥—ã",
    "–ì–æ–Ω–æ—á–Ω—ã–µ",
    "–ò–≥—Ä—ã —Å AR",
    "–ì–æ–ª–æ–≤–æ–ª–æ–º–∫–∏",
    "–°–ª–æ–≤–µ—Å–Ω—ã–µ",
    "–í–∏–∫—Ç–æ—Ä–∏–Ω—ã",
    "–ü—Ä–∏–∫–ª—é—á–µ–Ω–∏—è",
    "–†–æ–ª–µ–≤—ã–µ",
    "–ò–Ω–¥–∏",
    "–°—Ç—Ä–∞—Ç–µ–≥–∏–∏",
    "–ù–∞—Å—Ç–æ–ª—å–Ω—ã–µ –∏–≥—Ä—ã",
    "–ö–∞—Ä—Ç–æ—á–Ω—ã–µ",
    "–î–µ—Ç—Å–∫–∏–µ",
    "–°–µ–º–µ–π–Ω—ã–µ",
]
TARGET_CATEGORIES_LOWER = [cat.lower() for cat in TARGET_CATEGORIES]
OUTPUT_FILE = "filtered_apps.json"
REQUEST_DELAY = 0.8

def clean_text(text):
    if not text:
        return ""
    text = re.sub(r"[ \t]+", " ", text)
    text = re.sub(r"\n{3,}", "\n\n", text)
    return text.strip()

def is_target_category(categories):
    for cat in categories:
        if cat.lower() in TARGET_CATEGORIES_LOWER:
            return True
    return False

def parse_rustore_app(url, app_id):
    try:
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
            "Accept-Language": "ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7",
            "Referer": "https://www.rustore.ru/",
        }
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        response.encoding = "utf-8"

        soup = BeautifulSoup(response.text, "html.parser")

        script_tag = soup.find("script", type="application/ld+json")
        if not script_tag:
            return None

        try:
            data = json.loads(script_tag.string)
        except json.JSONDecodeError:
            return None

        app_data = None
        for item in data.get("@graph", []):
            if item.get("@type") == "SoftwareApplication":
                app_data = item
                break

        if not app_data:
            return None

        short_desc = clean_text(app_data.get("description", ""))
        full_desc = extract_full_description(soup)
        icon_url = app_data.get("image", "")
        developer = extract_developer_info(app_data)
        age_rating = app_data.get("typicalAgeRange", "–ù–µ —É–∫–∞–∑–∞–Ω")

        result = {
            "app_id": app_id,
            "name": clean_text(app_data.get("name", "")),
            "short_description": short_desc,
            "full_description": full_desc,
            "rating": parse_rating(app_data),
            "rating_count": parse_rating_count(app_data),
            "categories": parse_categories(app_data),
            "icon_url": icon_url,
            "screenshots": app_data.get("screenshot", []),
            "developer": developer,
            "age_rating": age_rating,
        }

        return result

    except Exception as e:
        return None

def extract_developer_info(app_data):
    author = app_data.get("author", {})
    if isinstance(author, dict):
        return {
            "name": clean_text(author.get("name", "–ù–µ —É–∫–∞–∑–∞–Ω")),
            "url": author.get("url", ""),
        }
    elif isinstance(author, str):
        return {"name": clean_text(author), "url": ""}
    else:
        return {"name": "–ù–µ —É–∫–∞–∑–∞–Ω", "url": ""}

def extract_full_description(soup):
    description_blocks = soup.find_all("div", {"data-testid": "description"})
    if not description_blocks:
        return "–ü–æ–¥—Ä–æ–±–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ"

    all_paragraphs = []
    for block in description_blocks:
        paragraphs = block.find_all("p", class_=re.compile(r"Pg0h2jm"))
        for p in paragraphs:
            paragraph_text = process_paragraph_content(p)
            if paragraph_text.strip():
                all_paragraphs.append(paragraph_text)

    return "\n\n".join(all_paragraphs)

def process_paragraph_content(element):
    text_parts = []
    for child in element.children:
        if isinstance(child, NavigableString):
            text = str(child)
            text = text.replace("\xa0", " ")
            text = re.sub(r"\s+", " ", text)
            text_parts.append(text)
        elif isinstance(child, Tag):
            if child.name == "br":
                text_parts.append("\n")
            else:
                text_parts.append(process_paragraph_content(child))

    return clean_text(" ".join(text_parts))

def parse_rating(app_data):
    try:
        rating = app_data.get("aggregateRating", {}).get("ratingValue")
        if rating is None:
            return 0.0
        return float(rating)
    except (TypeError, ValueError):
        return 0.0

def parse_rating_count(app_data):
    try:
        count = app_data.get("aggregateRating", {}).get("ratingCount")
        if count is None:
            return 0
        return int(count)
    except (TypeError, ValueError):
        return 0

def parse_categories(app_data):
    categories = app_data.get("applicationSubCategory", [])
    if isinstance(categories, str):
        return [clean_text(categories)]
    return [clean_text(cat) for cat in categories if cat.strip()]

def get_app_ids_from_catalog_page(url):
    try:
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
            "Accept-Language": "ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7",
        }
        response = requests.get(url, headers=headers, timeout=8)
        response.raise_for_status()
        response.encoding = "utf-8"

        soup = BeautifulSoup(response.text, "html.parser")
        app_cards = soup.find_all("a", {"data-testid": "app-card"})

        app_ids = []
        for card in app_cards:
            href = card.get("href", "")
            if href.startswith("/catalog/app/"):
                app_id = href.replace("/catalog/app/", "")
                app_ids.append(app_id)

        return app_ids

    except Exception as e:
        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {url}: {str(e)}")
        return []

def main():
    print(f"üöÄ –ù–∞—á–∏–Ω–∞–µ–º —Å–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö —Å {MAX_PAGES} —Å—Ç—Ä–∞–Ω–∏—Ü –∫–∞—Ç–∞–ª–æ–≥–∞ RuStore")

    all_app_ids = []
    for page_num in range(1, MAX_PAGES + 1):
        if page_num == 1:
            catalog_url = BASE_CATALOG_URL
        else:
            catalog_url = f"{BASE_CATALOG_URL}/page-{page_num}"

        print(f"üîÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∫–∞—Ç–∞–ª–æ–≥–∞: {catalog_url}")
        app_ids = get_app_ids_from_catalog_page(catalog_url)

        print(f"üîç –ù–∞–π–¥–µ–Ω–æ {len(app_ids)} –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page_num}")
        all_app_ids.extend(app_ids)

        time.sleep(REQUEST_DELAY)

    print(f"\n‚úÖ –í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ {len(all_app_ids)} –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏")

    filtered_apps = []
    print("\nüîç –ù–∞—á–∏–Ω–∞–µ–º –ø—Ä–æ–≤–µ—Ä–∫—É –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º...")

    for app_id in tqdm(all_app_ids, desc="–ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π"):
        app_url = f"https://www.rustore.ru/catalog/app/{app_id}"
        app_data = parse_rustore_app(app_url, app_id)

        if app_data and is_target_category(app_data["categories"]):
            filtered_apps.append(app_data)

        time.sleep(REQUEST_DELAY)

    print(f"\nüéØ –ù–∞–π–¥–µ–Ω–æ {len(filtered_apps)} –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏—Ö –∫—Ä–∏—Ç–µ—Ä–∏—è–º")

    if filtered_apps:
        with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
            json.dump(filtered_apps, f, ensure_ascii=False, indent=2)

        category_stats = {}
        for app in filtered_apps:
            for cat in app["categories"]:
                if cat in TARGET_CATEGORIES:
                    category_stats[cat] = category_stats.get(cat, 0) + 1

        print("\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º:")
        for cat, count in category_stats.items():
            print(f"- {cat}: {count} –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π")

        print(f"\nüíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ —Ñ–∞–π–ª: {OUTPUT_FILE}")

        print("\nüìå –ü—Ä–∏–º–µ—Ä—ã –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π:")
        for i, app in enumerate(filtered_apps[:3], 1):
            print(f"{i}. {app['name']} (ID: {app['app_id']})")
            print(f"   –ö–∞—Ç–µ–≥–æ—Ä–∏–∏: {', '.join(app['categories'])}")
            print(f"   –†–µ–π—Ç–∏–Ω–≥: {app['rating']} ({app['rating_count']} –æ—Ü–µ–Ω–æ–∫)")
            print(f"   –í–æ–∑—Ä–∞—Å—Ç–Ω–æ–π —Ä–µ–π—Ç–∏–Ω–≥: {app['age_rating']}")
            print(f"   –†–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫: {app['developer']['name']}")
            if app["developer"]["url"]:
                print(f"   –°—Ç—Ä–∞–Ω–∏—Ü–∞ —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–∞: {app['developer']['url']}")
            print(f"   –ò–∫–æ–Ω–∫–∞: {app['icon_url']}")
            print(f"   –ö—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ: {app['short_description'][:100]}{'...' if len(app['short_description']) > 100 else ''}")
    else:
        print("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ –Ω–∏ –æ–¥–Ω–æ–≥–æ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–µ–≥–æ –∫—Ä–∏—Ç–µ—Ä–∏—è–º")

if __name__ == "__main__":
    main()